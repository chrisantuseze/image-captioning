{"cells":[{"cell_type":"code","execution_count":null,"metadata":{"id":"daf9698e"},"outputs":[],"source":["import os  # when loading file paths\n","import pandas as pd  # for lookup in annotation file\n","import spacy  # for tokenizer\n","import torch\n","import torch.nn as nn\n","import torch.optim as optim\n","from torch.nn.utils.rnn import pad_sequence  # pad batch\n","from torch.utils.data import DataLoader, Dataset\n","from PIL import Image  # Load img\n","import torchvision.transforms as transforms\n","import torchvision.models as models\n","from tqdm import tqdm\n","from torch.utils.tensorboard import SummaryWriter\n","# from utils import save_checkpoint, load_checkpoint, print_examples\n","# from get_loader import get_loader\n","# from model import CNNtoRNN\n","\n","import zipfile\n"],"id":"daf9698e"},{"cell_type":"code","execution_count":null,"metadata":{"id":"EniISB_Gwjhc"},"outputs":[],"source":["# !pip install -U pip setuptools wheel\n","# !pip install -U spacy"],"id":"EniISB_Gwjhc"},{"cell_type":"code","execution_count":null,"metadata":{"id":"KC0smecOwkYN"},"outputs":[],"source":["# ! python -m spacy download en_core_web_sm"],"id":"KC0smecOwkYN"},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":26906,"status":"ok","timestamp":1659944341134,"user":{"displayName":"Chrisantus Eze","userId":"01021825377091372284"},"user_tz":300},"id":"SJTQmJmP_nKt","outputId":"d1fc001a-0e93-47b0-80d9-d016aac83dd8"},"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /content/drive\n"]}],"source":["from google.colab import drive\n","drive.mount('/content/drive')"],"id":"SJTQmJmP_nKt"},{"cell_type":"code","execution_count":null,"metadata":{"id":"KJg4cg023q97"},"outputs":[],"source":["path = \"/content/drive/MyDrive/AI/Colabs/CV/image-captioning/using-RNN/\"\n","# with zipfile.ZipFile(path + \"flickr8k.zip\",'r') as z:\n","#     z.extractall(path + \"flickr8k/\")"],"id":"KJg4cg023q97"},{"cell_type":"code","execution_count":null,"metadata":{"id":"QC9EXXTL3t7j"},"outputs":[],"source":[""],"id":"QC9EXXTL3t7j"},{"cell_type":"code","execution_count":null,"metadata":{"id":"3807b8ba"},"outputs":[],"source":["spacy_eng = spacy.load(\"en_core_web_sm\")"],"id":"3807b8ba"},{"cell_type":"code","execution_count":null,"metadata":{"id":"c483cca4"},"outputs":[],"source":["class Vocabulary:\n","    def __init__(self, freq_threshold):\n","        self.itos = {0: \"<PAD>\", 1: \"<SOS>\", 2: \"<EOS>\", 3: \"<UNK\"}\n","        self.stoi = {\"<PAD>\": 0, \"<SOS>\": 1, \"<EOS>\": 2, \"<UNK\": 3}\n","        self.freq_threshold = freq_threshold\n","        \n","    def __len__(self):\n","        return len(self.itos)\n","    \n","    @staticmethod\n","    def tokenizer_eng(text):\n","        return [tok.text.lower() for tok in spacy_eng.tokenizer(text)]\n","    \n","    def build_vocabulary(self, sentence_list):\n","        frequencies = {}\n","        idx = 4\n","        \n","        for sentence in sentence_list:\n","            for word in self.tokenizer_eng(sentence):\n","                if word not in frequencies:\n","                    frequencies[word] = 1\n","                    \n","                else:\n","                    frequencies[word] += 1\n","                    \n","                if frequencies[word] == self.freq_threshold:\n","                    self.stoi[word] = idx\n","                    self.itos[idx] = word\n","                    idx += 1\n","                    \n","    def numericalize(self, text):\n","        tokenized_text = self.tokenizer_eng(text)\n","        \n","        return [\n","            self.stoi[token] if token in self.stoi else self.stoi[\"<UNK\"] for token in tokenized_text\n","        ]"],"id":"c483cca4"},{"cell_type":"code","execution_count":null,"metadata":{"id":"b4da8383"},"outputs":[],"source":["class FlickrDataset(Dataset):\n","    def __init__(self, root_dir, captions_file, transform=None, freq_threshold=5):\n","        self.root_dir = root_dir\n","        self.df = pd.read_csv(captions_file)\n","        self.transform = transform\n","        \n","        self.imgs = self.df[\"image\"]\n","        self.captions = self.df[\"caption\"]\n","        \n","        self.vocab = Vocabulary(freq_threshold)\n","        self.vocab.build_vocabulary(self.captions.tolist())\n","        \n","    def __len__(self):\n","        return len(self.df)\n","    \n","    def __getitem__(self, index):\n","        img, caption = self.get_image_caption_pair(index)\n","\n","        if self.transform is not None:\n","            img = self.transform(img)\n","            \n","        numericalized_caption = [self.vocab.stoi[\"<SOS>\"]]\n","        numericalized_caption += self.vocab.numericalize(caption)\n","        numericalized_caption.append(self.vocab.stoi[\"<EOS>\"])\n","        \n","        return img, torch.tensor(numericalized_caption)\n","\n","    def get_image_caption_pair(self, index):\n","        caption = self.captions[index]\n","        img_id = self.imgs[index]\n","\n","        try:\n","          img = Image.open(os.path.join(self.root_dir, img_id)).convert(\"RGB\")\n","\n","        except:\n","          img, caption = self.get_image_caption_pair(index + 1)\n","        \n","        return img, caption"],"id":"b4da8383"},{"cell_type":"code","execution_count":null,"metadata":{"id":"e3be5774"},"outputs":[],"source":["class Collate:\n","    def __init__(self, pad_idx):\n","        self.pad_idx = pad_idx\n","        \n","    def __call__(self, batch):\n","        imgs = [item[0].unsqueeze(0) for item in batch]\n","        imgs = torch.cat(imgs, dim=0)\n","        targets = [item[1] for item in batch]\n","        targets = pad_sequence(targets, batch_first=False, padding_value=self.pad_idx)\n","        \n","        return imgs, targets"],"id":"e3be5774"},{"cell_type":"code","execution_count":null,"metadata":{"id":"d6f07937"},"outputs":[],"source":[""],"id":"d6f07937"},{"cell_type":"code","execution_count":null,"metadata":{"id":"40b2faaf"},"outputs":[],"source":["class EncoderCNN(nn.Module):\n","    def __init__(self, embed_size, train_CNN=False):\n","        super(EncoderCNN, self).__init__()\n","        self.train_CNN = train_CNN\n","        self.inception = models.inception_v3(weights=models.Inception_V3_Weights.DEFAULT)\n","        self.inception.fc = nn.Linear(self.inception.fc.in_features, embed_size)\n","        self.relu = nn.ReLU()\n","        self.times = []\n","        self.dropout = nn.Dropout(0.5)\n","        \n","    def forward(self, images):\n","        features = self.inception(images)\n","\n","        # Only finetune the CNN\n","        for name, param in self.inception.named_parameters():\n","          if \"fc.weight\" in name or \"fc.bias\" in name:\n","            param.requires_grad = True\n","          else:\n","            param.requires_grad = self.train_CNN\n","\n","        if type(features) == models.InceptionOutputs:\n","          features = features.logits\n","        return self.dropout(self.relu(features))"],"id":"40b2faaf"},{"cell_type":"code","execution_count":null,"metadata":{"id":"e72b15aa"},"outputs":[],"source":["class DecoderRNN(nn.Module):\n","    def __init__(self, embed_size, hidden_size, vocab_size, num_layers):\n","        super(DecoderRNN, self).__init__()\n","        \n","        self.embed = nn.Embedding(vocab_size, embed_size)\n","        self.lstm = nn.LSTM(embed_size, hidden_size, num_layers)\n","        self.linear = nn.Linear(hidden_size, vocab_size)\n","        self.dropout = nn.Dropout(0.5)\n","        \n","    def forward(self, features, captions):\n","        embeddings = self.dropout(self.embed(captions))\n","        embeddings = torch.cat((features.unsqueeze(0), embeddings), dim=0)\n","        hiddens, _ = self.lstm(embeddings)\n","        outputs = self.linear(hiddens)\n","        return outputs"],"id":"e72b15aa"},{"cell_type":"code","execution_count":null,"metadata":{"id":"80056abe"},"outputs":[],"source":["class CNNtoRNN(nn.Module):\n","    def __init__(self, embed_size, hidden_size, vocab_size, num_layers):\n","        super(CNNtoRNN, self).__init__()\n","        self.encoderCNN = EncoderCNN(embed_size)\n","        self.decoderRNN = DecoderRNN(embed_size, hidden_size, vocab_size, num_layers)\n","        \n","    def forward(self, images, captions):\n","        features = self.encoderCNN(images)\n","        outputs = self.decoderRNN(features, captions)\n","        return outputs\n","    \n","    def caption_image(self, image, vocabulary, max_length=50):\n","        result_caption = []\n","\n","        with torch.no_grad():\n","            x = self.encoderCNN(image).unsqueeze(0)\n","            states = None\n","\n","            for _ in range(max_length):\n","                hiddens, states = self.decoderRNN.lstm(x, states)\n","                output = self.decoderRNN.linear(hiddens.squeeze(0))\n","                predicted = output.argmax(1)\n","                result_caption.append(predicted.item())\n","                x = self.decoderRNN.embed(predicted).unsqueeze(0)\n","\n","                if vocabulary.itos[predicted.item()] == \"<EOS>\":\n","                    break\n","\n","        return [vocabulary.itos[idx] for idx in result_caption]"],"id":"80056abe"},{"cell_type":"code","execution_count":null,"metadata":{"id":"84074563"},"outputs":[],"source":[""],"id":"84074563"},{"cell_type":"code","execution_count":null,"metadata":{"id":"7S7CMU-4QFOe"},"outputs":[],"source":["def get_loader(\n","root_folder,\n","annotation_file,\n","transform,\n","batch_size=32,\n","num_workers=8,\n","shuffle=True,\n","pin_memory=True,):\n","    \n","    dataset = FlickrDataset(root_folder, annotation_file, transform=transform)\n","    pad_idx = dataset.vocab.stoi[\"<PAD>\"]\n","    \n","    loader = DataLoader(\n","    dataset=dataset,\n","    batch_size=batch_size,\n","    num_workers=num_workers,\n","    shuffle=shuffle,\n","    pin_memory=pin_memory,\n","    collate_fn=Collate(pad_idx=pad_idx),)\n","    \n","    return loader, dataset"],"id":"7S7CMU-4QFOe"},{"cell_type":"code","execution_count":null,"metadata":{"id":"lk8CdpY0QJ4r"},"outputs":[],"source":["def save_checkpoint(state, epoch, filename=\"my_checkpoint.pth.tar\"):\n","    print(f\"=> Saving checkpoint. Epoch: {epoch}\")\n","    torch.save(state, path + filename)\n","\n","\n","def load_checkpoint(checkpoint, model, optimizer):\n","    print(\"=> Loading checkpoint\")\n","    model.load_state_dict(checkpoint[\"state_dict\"])\n","    optimizer.load_state_dict(checkpoint[\"optimizer\"])\n","    step = checkpoint[\"step\"]\n","    return step"],"id":"lk8CdpY0QJ4r"},{"cell_type":"code","execution_count":null,"metadata":{"id":"TB8XAvVHnLIi"},"outputs":[],"source":["def print_examples(model, device, dataset):\n","    transform = transforms.Compose(\n","        [\n","            transforms.Resize((299, 299)),\n","            transforms.ToTensor(),\n","            transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5)),\n","        ]\n","    )\n","\n","    model.eval()\n","    test_img1 = transform(Image.open(path + \"test_examples/dog.jpeg\").convert(\"RGB\")).unsqueeze(\n","        0\n","    )\n","    print(\"Example 1 CORRECT: Dog on a beach by the ocean\")\n","    print(\n","        \"Example 1 OUTPUT: \"\n","        + \" \".join(model.caption_image(test_img1.to(device), dataset.vocab))\n","    )\n","\n","    test_img2 = transform(\n","        Image.open(path + \"test_examples/child.jpeg\").convert(\"RGB\")\n","    ).unsqueeze(0)\n","    print(\"Example 2 CORRECT: Child holding red frisbee outdoors\")\n","    print(\n","        \"Example 2 OUTPUT: \"\n","        + \" \".join(model.caption_image(test_img2.to(device), dataset.vocab))\n","    )\n","\n","    test_img3 = transform(Image.open(path + \"test_examples/bus.png\").convert(\"RGB\")).unsqueeze(\n","        0\n","    )\n","    print(\"Example 3 CORRECT: Bus driving by parked cars\")\n","    print(\n","        \"Example 3 OUTPUT: \"\n","        + \" \".join(model.caption_image(test_img3.to(device), dataset.vocab))\n","    )\n","\n","    test_img4 = transform(\n","        Image.open(path + \"test_examples/boat.png\").convert(\"RGB\")\n","    ).unsqueeze(0)\n","    print(\"Example 4 CORRECT: A small boat in the ocean\")\n","    print(\n","        \"Example 4 OUTPUT: \"\n","        + \" \".join(model.caption_image(test_img4.to(device), dataset.vocab))\n","    )\n","    \n","    test_img5 = transform(\n","        Image.open(path + \"test_examples/horse.png\").convert(\"RGB\")\n","    ).unsqueeze(0)\n","    print(\"Example 5 CORRECT: A cowboy riding a horse in the desert\")\n","    print(\n","        \"Example 5 OUTPUT: \"\n","        + \" \".join(model.caption_image(test_img5.to(device), dataset.vocab))\n","    )\n","    model.train()"],"id":"TB8XAvVHnLIi"},{"cell_type":"code","execution_count":null,"metadata":{"id":"08b675cf"},"outputs":[],"source":["# # !pip install cudnn=7.1.2\n","\n","# # When you get this error \"RuntimeError: cuDNN version incompatibility: PyTorch was compiled against (8, 3, 2) but linked against (8, 0, 5)\"\n","# # Run the below lines:\n","\n","!wget -O mini.sh https://repo.anaconda.com/miniconda/Miniconda3-py39_4.9.2-Linux-x86_64.sh\n","!chmod +x mini.sh\n","!bash ./mini.sh -b -f -p /usr/local\n","!conda install -q -y jupyter\n","!conda install -q -y google-colab -c conda-forge\n","!python -m ipykernel install --name \"py39\" --user"],"id":"08b675cf"},{"cell_type":"code","execution_count":null,"metadata":{"id":"3a691f70","colab":{"base_uri":"https://localhost:8080/"},"outputId":"67032b1c-59ae-42d0-bb61-53a788659090"},"outputs":[{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Batches in Train Loader: 1265\n","Examples in Train Loader: 40455\n","Size of Dataset: 40455\n","The runtime is cuda\n","=> Loading checkpoint\n","Example 1 CORRECT: Dog on a beach by the ocean\n","Example 1 OUTPUT: <SOS> a dog is running through the grass . <EOS>\n","Example 2 CORRECT: Child holding red frisbee outdoors\n","Example 2 OUTPUT: <SOS> a girl in a pink shirt is standing on a beach . <EOS>\n","Example 3 CORRECT: Bus driving by parked cars\n","Example 3 OUTPUT: <SOS> a man in a black shirt and a black shirt is standing on a bench with a <UNK in his hand . <EOS>\n","Example 4 CORRECT: A small boat in the ocean\n","Example 4 OUTPUT: <SOS> a man in a red jacket is standing on a rock overlooking a lake . <EOS>\n","Example 5 CORRECT: A cowboy riding a horse in the desert\n","Example 5 OUTPUT: <SOS> a dog is running through the grass . <EOS>\n","=> Saving checkpoint. Epoch: 0\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":[""]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Example 1 CORRECT: Dog on a beach by the ocean\n","Example 1 OUTPUT: <SOS> a dog is running through the grass . <EOS>\n","Example 2 CORRECT: Child holding red frisbee outdoors\n","Example 2 OUTPUT: <SOS> a young boy in a blue shirt is playing with a ball in the grass . <EOS>\n","Example 3 CORRECT: Bus driving by parked cars\n","Example 3 OUTPUT: <SOS> a man in a black shirt is standing on a bench with a <UNK in his mouth . <EOS>\n","Example 4 CORRECT: A small boat in the ocean\n","Example 4 OUTPUT: <SOS> a man in a red shirt is standing on a rock overlooking a lake . <EOS>\n","Example 5 CORRECT: A cowboy riding a horse in the desert\n","Example 5 OUTPUT: <SOS> a dog is running through the grass . <EOS>\n","=> Saving checkpoint. Epoch: 1\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":[""]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Example 1 CORRECT: Dog on a beach by the ocean\n","Example 1 OUTPUT: <SOS> a dog is running through the grass . <EOS>\n","Example 2 CORRECT: Child holding red frisbee outdoors\n","Example 2 OUTPUT: <SOS> a young girl in a pink shirt is playing with a toy in the grass . <EOS>\n","Example 3 CORRECT: Bus driving by parked cars\n","Example 3 OUTPUT: <SOS> a man in a black shirt and a black shirt is standing on a bench . <EOS>\n","Example 4 CORRECT: A small boat in the ocean\n","Example 4 OUTPUT: <SOS> a man in a blue shirt is standing on a rock overlooking a lake . <EOS>\n","Example 5 CORRECT: A cowboy riding a horse in the desert\n","Example 5 OUTPUT: <SOS> a dog is running through the snow . <EOS>\n","=> Saving checkpoint. Epoch: 2\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":[""]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Example 1 CORRECT: Dog on a beach by the ocean\n","Example 1 OUTPUT: <SOS> a dog runs through the grass . <EOS>\n","Example 2 CORRECT: Child holding red frisbee outdoors\n","Example 2 OUTPUT: <SOS> a little girl in a pink shirt is standing on a sidewalk . <EOS>\n","Example 3 CORRECT: Bus driving by parked cars\n","Example 3 OUTPUT: <SOS> a man in a black shirt and a black shirt is standing on a sidewalk . <EOS>\n","Example 4 CORRECT: A small boat in the ocean\n","Example 4 OUTPUT: <SOS> a man is climbing a rock wall . <EOS>\n","Example 5 CORRECT: A cowboy riding a horse in the desert\n","Example 5 OUTPUT: <SOS> a dog is running through the snow . <EOS>\n","=> Saving checkpoint. Epoch: 3\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":[""]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Example 1 CORRECT: Dog on a beach by the ocean\n","Example 1 OUTPUT: <SOS> a dog is running through the snow . <EOS>\n","Example 2 CORRECT: Child holding red frisbee outdoors\n","Example 2 OUTPUT: <SOS> a little girl in a pink shirt is playing with a toy in the grass . <EOS>\n","Example 3 CORRECT: Bus driving by parked cars\n","Example 3 OUTPUT: <SOS> a man in a red shirt is standing on a bench in front of a building . <EOS>\n","Example 4 CORRECT: A small boat in the ocean\n","Example 4 OUTPUT: <SOS> a man in a blue shirt is standing on a rock overlooking the ocean . <EOS>\n","Example 5 CORRECT: A cowboy riding a horse in the desert\n","Example 5 OUTPUT: <SOS> a man and a woman are walking on a beach . <EOS>\n","=> Saving checkpoint. Epoch: 4\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":[""]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Example 1 CORRECT: Dog on a beach by the ocean\n","Example 1 OUTPUT: <SOS> a dog is running through the snow . <EOS>\n","Example 2 CORRECT: Child holding red frisbee outdoors\n","Example 2 OUTPUT: <SOS> a little girl in a pink shirt is standing on a sidewalk . <EOS>\n","Example 3 CORRECT: Bus driving by parked cars\n","Example 3 OUTPUT: <SOS> a man in a red shirt is standing on a bench with a <UNK in his hand . <EOS>\n","Example 4 CORRECT: A small boat in the ocean\n","Example 4 OUTPUT: <SOS> a man in a wetsuit is surfing a wave . <EOS>\n","Example 5 CORRECT: A cowboy riding a horse in the desert\n","Example 5 OUTPUT: <SOS> a dog is running through the snow . <EOS>\n","=> Saving checkpoint. Epoch: 5\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":[""]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Example 1 CORRECT: Dog on a beach by the ocean\n","Example 1 OUTPUT: <SOS> a dog runs through the water . <EOS>\n","Example 2 CORRECT: Child holding red frisbee outdoors\n","Example 2 OUTPUT: <SOS> a little boy in a red shirt is jumping into a pool . <EOS>\n","Example 3 CORRECT: Bus driving by parked cars\n","Example 3 OUTPUT: <SOS> a man is standing on a bench in front of a building . <EOS>\n","Example 4 CORRECT: A small boat in the ocean\n","Example 4 OUTPUT: <SOS> a man in a red shirt is standing on a rock overlooking the ocean . <EOS>\n","Example 5 CORRECT: A cowboy riding a horse in the desert\n","Example 5 OUTPUT: <SOS> a dog is running through the snow . <EOS>\n","=> Saving checkpoint. Epoch: 6\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":[""]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Example 1 CORRECT: Dog on a beach by the ocean\n","Example 1 OUTPUT: <SOS> a dog is running through the snow . <EOS>\n","Example 2 CORRECT: Child holding red frisbee outdoors\n","Example 2 OUTPUT: <SOS> a little girl in a pink shirt is playing with a tennis ball . <EOS>\n","Example 3 CORRECT: Bus driving by parked cars\n","Example 3 OUTPUT: <SOS> a man in a red shirt is standing on a sidewalk . <EOS>\n","Example 4 CORRECT: A small boat in the ocean\n","Example 4 OUTPUT: <SOS> a man is surfing a wave . <EOS>\n","Example 5 CORRECT: A cowboy riding a horse in the desert\n","Example 5 OUTPUT: <SOS> a man and a woman are walking on a beach . <EOS>\n","=> Saving checkpoint. Epoch: 7\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":[""]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Example 1 CORRECT: Dog on a beach by the ocean\n","Example 1 OUTPUT: <SOS> a dog is running through the snow . <EOS>\n","Example 2 CORRECT: Child holding red frisbee outdoors\n","Example 2 OUTPUT: <SOS> a little girl in a pink shirt is jumping into a pool . <EOS>\n","Example 3 CORRECT: Bus driving by parked cars\n","Example 3 OUTPUT: <SOS> a man is standing on a sidewalk with his arms folded . <EOS>\n","Example 4 CORRECT: A small boat in the ocean\n","Example 4 OUTPUT: <SOS> a man is standing on a rocky shore . <EOS>\n","Example 5 CORRECT: A cowboy riding a horse in the desert\n","Example 5 OUTPUT: <SOS> a dog is running through the snow . <EOS>\n","=> Saving checkpoint. Epoch: 8\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":[""]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Example 1 CORRECT: Dog on a beach by the ocean\n","Example 1 OUTPUT: <SOS> a dog is running through the snow . <EOS>\n","Example 2 CORRECT: Child holding red frisbee outdoors\n","Example 2 OUTPUT: <SOS> a little girl in a pink shirt is playing with a toy in the grass . <EOS>\n","Example 3 CORRECT: Bus driving by parked cars\n","Example 3 OUTPUT: <SOS> a man is standing on a sidewalk with a <UNK in his hand . <EOS>\n","Example 4 CORRECT: A small boat in the ocean\n","Example 4 OUTPUT: <SOS> a man is surfing on a surfboard . <EOS>\n","Example 5 CORRECT: A cowboy riding a horse in the desert\n","Example 5 OUTPUT: <SOS> a man and a woman are walking on a beach . <EOS>\n","=> Saving checkpoint. Epoch: 9\n"]},{"output_type":"stream","name":"stderr","text":[""]},{"output_type":"stream","name":"stdout","text":["Example 1 CORRECT: Dog on a beach by the ocean\n","Example 1 OUTPUT: <SOS> a brown dog is running through the water . <EOS>\n","Example 2 CORRECT: Child holding red frisbee outdoors\n","Example 2 OUTPUT: <SOS> a little girl in a pink shirt is jumping over a hurdle . <EOS>\n","Example 3 CORRECT: Bus driving by parked cars\n","Example 3 OUTPUT: <SOS> a man in a red shirt is standing on a sidewalk with a <UNK in the background . <EOS>\n","Example 4 CORRECT: A small boat in the ocean\n","Example 4 OUTPUT: <SOS> a man in a blue shirt is surfing on a surfboard . <EOS>\n","Example 5 CORRECT: A cowboy riding a horse in the desert\n","Example 5 OUTPUT: <SOS> a man and a woman are walking through a snowy field . <EOS>\n","=> Saving checkpoint. Epoch: 10\n"]},{"output_type":"stream","name":"stderr","text":["  7%|â–‹         | 93/1265 [00:28<06:26,  3.03it/s]"]}],"source":["def train():\n","    transform = transforms.Compose(\n","        [\n","            transforms.Resize((356, 356)),\n","            transforms.RandomCrop((299, 299)),\n","            transforms.ToTensor(),\n","            transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5)),\n","        ]\n","    )\n","\n","    train_loader, dataset = get_loader(\n","        root_folder= path + \"flickr8k/images\",\n","        annotation_file= path + \"flickr8k/captions.txt\",\n","        transform=transform,\n","        num_workers=2,\n","    )\n","\n","    print(\"Batches in Train Loader: {}\".format(len(train_loader)))\n","    print(\"Examples in Train Loader: {}\".format(len(train_loader.sampler)))\n","    print(\"Size of Dataset: {}\".format(len(dataset)))\n","\n","    torch.backends.cudnn.benchmark = True\n","    dev = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n","    print(f\"The runtime is {dev}\")\n","    device = torch.device(dev)\n","    load_model = True\n","    save_model = True\n","    train_CNN = False\n","\n","    # Hyperparameters\n","    embed_size = 256\n","    hidden_size = 256\n","    vocab_size = len(dataset.vocab)\n","    num_layers = 1\n","    learning_rate = 3e-4\n","    num_epochs = 50\n","\n","    # for tensorboard\n","    # writer = SummaryWriter(\"runs/flickr\")\n","    step = 0\n","\n","    # initialize model, loss etc\n","    model = CNNtoRNN(embed_size, hidden_size, vocab_size, num_layers)\n","    model = model.to(device)\n","\n","    criterion = nn.CrossEntropyLoss(ignore_index=dataset.vocab.stoi[\"<PAD>\"])\n","    optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n","\n","    if load_model:\n","      checkpoint = torch.load(path + \"my_checkpoint.pth.tar\")\n","      step = load_checkpoint(checkpoint, model, optimizer)\n","\n","    model.train()\n","\n","    for epoch in range(num_epochs):\n","        # Uncomment the line below to see a couple of test cases\n","        print_examples(model, device, dataset)\n","\n","        if save_model:\n","            checkpoint = {\n","                \"state_dict\": model.state_dict(),\n","                \"optimizer\": optimizer.state_dict(),\n","                \"step\": step,\n","            }\n","            save_checkpoint(checkpoint, epoch)\n","\n","        for idx, (imgs, captions) in tqdm(enumerate(train_loader), total=len(train_loader), leave=False):\n","            imgs = imgs.to(device)\n","            captions = captions.to(device)\n","\n","            outputs = model(imgs, captions[:-1])\n","            loss = criterion(outputs.reshape(-1, outputs.shape[2]), captions.reshape(-1))\n","\n","            # writer.add_scalar(\"Training loss\", loss.item(), global_step=step)\n","            step += 1\n","\n","            optimizer.zero_grad()\n","            loss.backward(loss)\n","            optimizer.step()\n","\n","if __name__ == \"__main__\":\n","    train()"],"id":"3a691f70"}],"metadata":{"colab":{"collapsed_sections":[],"name":"ic-using-RNN.ipynb","provenance":[]},"gpuClass":"standard","kernelspec":{"display_name":"Python 3 (ipykernel)","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.9.12"},"accelerator":"GPU"},"nbformat":4,"nbformat_minor":5}